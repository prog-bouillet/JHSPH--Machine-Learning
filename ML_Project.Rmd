---
title: "Coursera - JHSPH - Machine Learning Class Project"
author: "prog.bouillet"
date: "December 21, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr, warn.conflicts = FALSE)
library(caret, warn.conflicts = FALSE)
library(rpart, warn.conflicts = FALSE)

set.seed(1997)
```
## Introduction
In this project we build a classification model to predict if weightlifting is being done with 
good form based on relevant sensor data. The dataset is available [here](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har#literature).

You can find the **GIT** repository with the ".rmd" file used to generate this html report [here](https://github.com/prog-bouillet/JHSPH--Machine-Learning).

### Data Cleaning
The dataset has many NA variables and many variables with just plain empty data, so to clean it
up prior to processing, we eliminate those variables and arrive at a dataset with 54 of the 
original 93 variables remaining.

```{r}
a <- read.csv("pml-training.csv")

# remove columns without data values
var_avail <- colSums(is.na(a)) < 100
b <- as_tibble(a[,var_avail])

# remove irrelevant features
v <- select(b, -c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window))
z <- sapply(v,is.numeric)
cn <- colnames(v[z])
vsel <- select(v, cn)

# add class and name variables back
cleandat <- mutate(vsel, class=b$classe, user_name=b$user_name)

colnames(cleandat)
```

### Model Selection
This is a classification task, so a CART model is appropriate. I will implement the model
using **rpart()** from the **rpart** library. Since the raw sensor data is in the form
of time sequences for each lift, I originally tried to average the sensor readings for each
lift before creating the model, but this ended up not predicting well, so I just reverted
to building the model on the raw per-sample data. 

First, the basic data set partitioning into training and validation subsets:
```{r}
# divide into training and validation sets
inTrain = createDataPartition(cleandat$class, p = 0.80)[[1]]
training = cleandat[inTrain,]
testing = cleandat[-inTrain,]
```

Now just let the function work its magic:

```{r}
# rpart (recursive partitioning / classification & regression tree)
fitrp <- rpart(class ~ ., data = training, method = "class", control = rpart.control(cp = 0))
plotcp(fitrp)
```

I might think about pruning the tree if the number of involved variables was smaller, but it takes
a large tree to get to a low cross-validation error level, so it's probably not worth the effort
of pruning.

### Cross-Validation

So, let's look at the performance on testing data subset:

```{r}
prdrp <- predict(fitrp, testing, type = "class")
cmrp <- confusionMatrix(prdrp, testing$class)
print(cmrp)
```

I was focusing on positive predictive value, which seems decently high, so I think it will
probably do good enough to get me past the quiz.

Specifically, according to the overall accuracy metric, I should get 0.9319*20=18.6, or 18/19 of the 20 predictions right with high confidence (95%). (1-Accuracy), or 6-7% is basically the estimated out-of-sample error according to the test dataset.

### Predict quiz set

```{r}
aquiz <- read.csv("pml-testing.csv")

prdrp_quiz <- predict(fitrp, aquiz, type = "class")
#print(prdrp_quiz)
```

Well, I missed 2 on the quiz, so the out-of-sample error estimation was pretty accurate.

And I passed, so all's good.




